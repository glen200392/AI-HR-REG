#!/usr/bin/env python3
"""
ç¡¬é«”éœ€æ±‚åˆ†æå·¥å…·
åˆ†æä¸åŒLLMæ¨¡å‹çš„ç¡¬é«”éœ€æ±‚å’Œæˆæœ¬
"""

import json
from typing import Dict, List

class HardwareAnalyzer:
    """ç¡¬é«”éœ€æ±‚åˆ†æå™¨"""
    
    def __init__(self):
        self.models = {
            "Llama2-7B": {
                "parameters": "7B",
                "model_size_gb": 13,
                "min_ram_gb": 16,
                "recommended_ram_gb": 32,
                "min_vram_gb": 8,
                "recommended_vram_gb": 16,
                "cpu_cores": 8,
                "storage_gb": 50,
                "inference_speed": "medium",
                "tokens_per_second": 15
            },
            "Llama2-13B": {
                "parameters": "13B", 
                "model_size_gb": 26,
                "min_ram_gb": 32,
                "recommended_ram_gb": 64,
                "min_vram_gb": 16,
                "recommended_vram_gb": 24,
                "cpu_cores": 16,
                "storage_gb": 100,
                "inference_speed": "medium-slow",
                "tokens_per_second": 10
            },
            "Mistral-7B": {
                "parameters": "7B",
                "model_size_gb": 14,
                "min_ram_gb": 16,
                "recommended_ram_gb": 32,
                "min_vram_gb": 8,
                "recommended_vram_gb": 16,
                "cpu_cores": 8,
                "storage_gb": 50,
                "inference_speed": "fast",
                "tokens_per_second": 20
            },
            "Qwen-14B": {
                "parameters": "14B",
                "model_size_gb": 28,
                "min_ram_gb": 32,
                "recommended_ram_gb": 64,
                "min_vram_gb": 16,
                "recommended_vram_gb": 32,
                "cpu_cores": 16,
                "storage_gb": 100,
                "inference_speed": "medium-slow", 
                "tokens_per_second": 8
            }
        }
        
        self.hardware_costs = {
            "cpu_server": {
                "name": "é«˜éšCPUä¼ºæœå™¨",
                "specs": "32æ ¸å¿ƒCPU, 64GB RAM, 1TB SSD",
                "cost_usd": 3000,
                "monthly_power": 150,
                "suitable_models": ["Llama2-7B", "Mistral-7B"]
            },
            "gpu_server_mid": {
                "name": "ä¸­éšGPUä¼ºæœå™¨", 
                "specs": "16æ ¸å¿ƒCPU, 64GB RAM, RTX 4090 24GB",
                "cost_usd": 5000,
                "monthly_power": 400,
                "suitable_models": ["Llama2-13B", "Mistral-7B", "Llama2-7B"]
            },
            "gpu_server_high": {
                "name": "é«˜éšGPUä¼ºæœå™¨",
                "specs": "32æ ¸å¿ƒCPU, 128GB RAM, A100 80GB",
                "cost_usd": 15000,
                "monthly_power": 600,
                "suitable_models": ["Qwen-14B", "Llama2-13B", "All Models"]
            },
            "cloud_gpu": {
                "name": "é›²ç«¯GPUå¯¦ä¾‹",
                "specs": "A100 40GB (æŒ‰æ™‚è¨ˆè²»)",
                "cost_per_hour": 2.5,
                "monthly_cost_24x7": 1800,
                "suitable_models": ["All Models"]
            }
        }
    
    def analyze_model_requirements(self, model_name: str) -> Dict:
        """åˆ†æç‰¹å®šæ¨¡å‹çš„ç¡¬é«”éœ€æ±‚"""
        if model_name not in self.models:
            return {"error": f"Model {model_name} not found"}
        
        model = self.models[model_name]
        
        # æ‰¾å‡ºé©åˆçš„ç¡¬é«”é…ç½®
        suitable_hardware = []
        for hw_name, hw_spec in self.hardware_costs.items():
            if model_name in hw_spec["suitable_models"] or "All Models" in hw_spec["suitable_models"]:
                suitable_hardware.append({
                    "name": hw_spec["name"],
                    "specs": hw_spec["specs"],
                    "initial_cost": hw_spec.get("cost_usd", 0),
                    "monthly_cost": hw_spec.get("monthly_power", hw_spec.get("monthly_cost_24x7", 0))
                })
        
        return {
            "model": model_name,
            "requirements": model,
            "suitable_hardware": suitable_hardware,
            "performance_estimate": self.estimate_performance(model)
        }
    
    def estimate_performance(self, model_spec: Dict) -> Dict:
        """ä¼°ç®—æ€§èƒ½è¡¨ç¾"""
        return {
            "concurrent_users": max(1, model_spec["tokens_per_second"] // 5),
            "daily_analyses": model_spec["tokens_per_second"] * 60 * 60 * 8 // 100,  # 8å°æ™‚å·¥ä½œæ™‚é–“
            "response_time_seconds": 100 / model_spec["tokens_per_second"],
            "scalability": "High" if model_spec["tokens_per_second"] > 15 else "Medium"
        }
    
    def compare_total_cost_of_ownership(self, years: int = 3) -> Dict:
        """æ¯”è¼ƒç¸½é«”æ“æœ‰æˆæœ¬"""
        tco_analysis = {}
        
        for model_name in self.models.keys():
            model_analysis = self.analyze_model_requirements(model_name)
            
            if "error" in model_analysis:
                continue
            
            # é¸æ“‡æœ€ç¶“æ¿Ÿçš„ç¡¬é«”é…ç½®
            best_hardware = min(
                model_analysis["suitable_hardware"], 
                key=lambda x: x["initial_cost"] + x["monthly_cost"] * 12 * years
            )
            
            # è¨ˆç®—TCO
            initial_cost = best_hardware["initial_cost"]
            operational_cost = best_hardware["monthly_cost"] * 12 * years
            maintenance_cost = initial_cost * 0.1 * years  # 10% annual maintenance
            
            total_cost = initial_cost + operational_cost + maintenance_cost
            
            tco_analysis[model_name] = {
                "hardware": best_hardware["name"],
                "initial_cost": initial_cost,
                "operational_cost": operational_cost, 
                "maintenance_cost": maintenance_cost,
                "total_cost": total_cost,
                "cost_per_analysis": total_cost / (model_analysis["performance_estimate"]["daily_analyses"] * 365 * years),
                "performance": model_analysis["performance_estimate"]
            }
        
        return tco_analysis
    
    def generate_recommendation(self) -> Dict:
        """ç”Ÿæˆç¡¬é«”æ¨è–¦"""
        tco = self.compare_total_cost_of_ownership()
        
        # æŒ‰ä¸åŒæ¨™æº–æ’åº
        by_cost = sorted(tco.items(), key=lambda x: x[1]["total_cost"])
        by_performance = sorted(tco.items(), key=lambda x: x[1]["performance"]["tokens_per_second"], reverse=True)
        by_efficiency = sorted(tco.items(), key=lambda x: x[1]["cost_per_analysis"])
        
        return {
            "most_cost_effective": by_cost[0],
            "best_performance": by_performance[0] if by_performance else None,
            "best_efficiency": by_efficiency[0],
            "detailed_comparison": tco
        }

def main():
    analyzer = HardwareAnalyzer()
    
    print("ğŸ”§ ç¡¬é«”éœ€æ±‚åˆ†æå ±å‘Š")
    print("=" * 50)
    
    # åˆ†æå„æ¨¡å‹éœ€æ±‚
    for model in ["Llama2-7B", "Llama2-13B", "Mistral-7B", "Qwen-14B"]:
        print(f"\nğŸ“Š {model} åˆ†æ:")
        analysis = analyzer.analyze_model_requirements(model)
        
        if "error" not in analysis:
            req = analysis["requirements"]
            perf = analysis["performance_estimate"]
            
            print(f"  æ¨¡å‹å¤§å°: {req['model_size_gb']}GB")
            print(f"  æœ€å°RAM: {req['min_ram_gb']}GB")
            print(f"  æ¨è–¦RAM: {req['recommended_ram_gb']}GB")
            print(f"  æ¨ç†é€Ÿåº¦: {req['tokens_per_second']} tokens/ç§’")
            print(f"  ä¸¦ç™¼ç”¨æˆ¶: {perf['concurrent_users']} äºº")
            print(f"  æ—¥åˆ†æé‡: {perf['daily_analyses']} æ¬¡")
    
    # TCOåˆ†æ
    print(f"\nğŸ’° 3å¹´ç¸½é«”æ“æœ‰æˆæœ¬åˆ†æ:")
    print("=" * 50)
    
    tco = analyzer.compare_total_cost_of_ownership(3)
    for model, cost_data in tco.items():
        print(f"\n{model}:")
        print(f"  ç¡¬é«”: {cost_data['hardware']}")
        print(f"  åˆå§‹æˆæœ¬: ${cost_data['initial_cost']:,}")
        print(f"  ç‡Ÿé‹æˆæœ¬: ${cost_data['operational_cost']:,}")
        print(f"  ç¸½æˆæœ¬: ${cost_data['total_cost']:,}")
        print(f"  æ¯æ¬¡åˆ†ææˆæœ¬: ${cost_data['cost_per_analysis']:.4f}")
    
    # æ¨è–¦æ–¹æ¡ˆ
    print(f"\nğŸ¯ æ¨è–¦æ–¹æ¡ˆ:")
    print("=" * 50)
    
    recommendation = analyzer.generate_recommendation()
    
    print(f"æœ€å…·æˆæœ¬æ•ˆç›Š: {recommendation['most_cost_effective'][0]}")
    print(f"  ç¸½æˆæœ¬: ${recommendation['most_cost_effective'][1]['total_cost']:,}")
    
    if recommendation['best_performance']:
        print(f"æœ€ä½³æ€§èƒ½: {recommendation['best_performance'][0]}")
        print(f"  æ¨ç†é€Ÿåº¦: {recommendation['best_performance'][1]['performance']['tokens_per_second']} tokens/ç§’")
    
    print(f"æœ€é«˜æ•ˆç‡: {recommendation['best_efficiency'][0]}")
    print(f"  æ¯æ¬¡åˆ†ææˆæœ¬: ${recommendation['best_efficiency'][1]['cost_per_analysis']:.4f}")

if __name__ == "__main__":
    main()