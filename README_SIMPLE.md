# é«˜è³ªé‡HR AIåŠ©æ‰‹

ä¸€å€‹å°ˆæ³¨æ–¼å…§å®¹è³ªé‡å’Œæ˜“ç”¨æ€§çš„HRäººå·¥æ™ºèƒ½åˆ†æå·¥å…·ã€‚

## âœ¨ æ ¸å¿ƒç‰¹è‰²

- **å¤šLLMæ”¯æŒ**: OpenAIã€Claudeã€Groqã€Ollamaæœ¬åœ°æ¨¡å‹
- **å°ˆæ¥­HRåˆ†æ**: åŸºæ–¼15å¹´HRç¶“é©—çš„å°ˆæ¥­æç¤ºæ¨¡æ¿
- **è³ªé‡ä¿è­‰**: è‡ªå‹•è©•ä¼°å’Œæ”¹é€²åˆ†æè³ªé‡
- **ç¹é«”ä¸­æ–‡**: å®Œæ•´æ”¯æ´å°ç£HRè¡“èªå’Œè·å ´æ–‡åŒ–
- **å³æ’å³ç”¨**: 5åˆ†é˜å…§å®Œæˆå®‰è£å’Œä½¿ç”¨

## ğŸš€ å¿«é€Ÿé–‹å§‹

### 1. å®‰è£ä¾è³´

```bash
# æœ€å°å®‰è£ï¼ˆä½¿ç”¨å‚™ç”¨å›æ‡‰ï¼‰
pip install python-dotenv

# åŸºæœ¬å®‰è£ï¼ˆæ”¯æŒOllamaæœ¬åœ°æ¨¡å‹ï¼‰
pip install requests python-dotenv

# å®Œæ•´å®‰è£ï¼ˆæ”¯æŒæ‰€æœ‰LLMï¼‰
pip install -r simple_requirements.txt
```

### 2. é…ç½®LLMï¼ˆå¯é¸ï¼‰

```bash
# OpenAI
export OPENAI_API_KEY="your-api-key"
export LLM_PROVIDER="openai"

# Claude
export ANTHROPIC_API_KEY="your-api-key" 
export LLM_PROVIDER="claude"

# Ollamaï¼ˆæœ¬åœ°é‹è¡Œï¼‰
export LLM_PROVIDER="ollama"
export OLLAMA_MODEL="llama2:13b"

# Groq
export GROQ_API_KEY="your-api-key"
export LLM_PROVIDER="groq"
```

### 3. ç«‹å³ä½¿ç”¨

```bash
# åˆ†æå“¡å·¥
python hr_ai_quality.py analyze-employee examples/employee_sample.json

# åˆ†æåœ˜éšŠ  
python hr_ai_quality.py analyze-team examples/team_sample.json

# æŒ‡å®šè¼¸å‡ºæ–‡ä»¶
python hr_ai_quality.py analyze-employee examples/employee_sample.json --output-file result.json

# ä½¿ç”¨ç‰¹å®šæ¨¡å‹
python hr_ai_quality.py analyze-employee examples/employee_sample.json --provider ollama --model llama2:13b
```

## ğŸ“Š æ”¯æŒçš„LLMæä¾›å•†

| æä¾›å•† | æ¨è–¦æ¨¡å‹ | å„ªå‹¢ | æˆæœ¬ |
|--------|----------|------|------|
| **OpenAI** | gpt-4, gpt-3.5-turbo | æœ€ä½³ä¸­æ–‡ç†è§£ | ä¸­ç­‰ |
| **Claude** | claude-3-sonnet | å°ˆæ¥­åˆ†æèƒ½åŠ› | ä¸­ç­‰ |
| **Groq** | mixtral-8x7b | è¶…å¿«éŸ¿æ‡‰é€Ÿåº¦ | ä½ |
| **Ollama** | llama2:13b, mistral:7b | å®Œå…¨å…è²»æœ¬åœ°é‹è¡Œ | å…è²» |

## ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹

### å“¡å·¥åˆ†æç¤ºä¾‹

```json
{
  "id": "emp_001",
  "name": "ç‹å°æ˜",
  "department": "è»Ÿé«”å·¥ç¨‹éƒ¨",
  "role": "è³‡æ·±è»Ÿé«”å·¥ç¨‹å¸«", 
  "experience_years": 6,
  "skills": {
    "Python": 0.9,
    "é ˜å°èƒ½åŠ›": 0.7,
    "æºé€šå”èª¿": 0.8
  },
  "performance_score": 0.88
}
```

### åˆ†æçµæœç¤ºä¾‹

```json
{
  "employee_name": "ç‹å°æ˜",
  "analysis_timestamp": "2024-01-15T10:30:00",
  "detailed_analysis": "## äººæ‰ç‰¹è³ªè©•ä¼°\n\nç‹å°æ˜å±•ç¾å‡ºå„ªç§€çš„æŠ€è¡“èƒ½åŠ›...",
  "quality_assessment": {
    "quality_score": 0.85,
    "is_acceptable": true,
    "feedback": []
  },
  "llm_provider": "openai",
  "model_used": "gpt-4"
}
```

## ğŸ¯ è³ªé‡ä¿è­‰æ©Ÿåˆ¶

### è‡ªå‹•è³ªé‡è©•ä¼°

- **å…§å®¹é•·åº¦**: ç¢ºä¿åˆ†æè©³ç´°å®Œæ•´
- **çµæ§‹åŒ–ç¨‹åº¦**: æª¢æŸ¥æ ¼å¼å’Œçµ„ç¹”
- **å°ˆæ¥­è¡“èª**: é©—è­‰HRå°ˆæ¥­ç”¨è©
- **å¯åŸ·è¡Œæ€§**: è©•ä¼°å»ºè­°çš„å¯¦ç”¨æ€§

### è³ªé‡æ”¹é€²æµç¨‹

1. **åˆæ¬¡ç”Ÿæˆ**: LLMç”¢ç”Ÿåˆ†æå…§å®¹
2. **è³ªé‡è©•ä¼°**: è‡ªå‹•è©•åˆ†å’Œåé¥‹
3. **æ™ºèƒ½æ”¹é€²**: ä½è³ªé‡å…§å®¹è‡ªå‹•é‡æ–°ç”Ÿæˆ
4. **æœ€çµ‚è¼¸å‡º**: ç¢ºä¿è³ªé‡é”æ¨™

## ğŸ”§ é€²éšé…ç½®

### ç’°å¢ƒè®Šé‡

```bash
# LLMé…ç½®
export LLM_PROVIDER="openai"              # LLMæä¾›å•†
export LLM_TEMPERATURE="0.7"              # å‰µé€ æ€§åƒæ•¸
export OPENAI_MODEL="gpt-4"               # æŒ‡å®šæ¨¡å‹

# è³ªé‡æ§åˆ¶
export QUALITY_THRESHOLD="0.6"            # è³ªé‡é–¾å€¼
export MAX_RETRY_ATTEMPTS="2"             # æœ€å¤§é‡è©¦æ¬¡æ•¸

# Ollamaé…ç½®ï¼ˆæœ¬åœ°æ¨¡å‹ï¼‰
export OLLAMA_BASE_URL="http://localhost:11434"
export OLLAMA_MODEL="llama2:13b"
```

### å‘½ä»¤è¡Œé¸é …

```bash
python hr_ai_quality.py --help

# å¸¸ç”¨é¸é …
--provider {openai,claude,groq,ollama}     # æŒ‡å®šLLMæä¾›å•†
--model MODEL_NAME                         # æŒ‡å®šæ¨¡å‹åç¨±
--quality-threshold 0.8                    # è¨­å®šè³ªé‡é–¾å€¼
--output-file results.json                 # è¼¸å‡ºæ–‡ä»¶
```

## ğŸ“ˆ æ€§èƒ½å°æ¯”

| æ¶æ§‹ | ä»£ç¢¼è¡Œæ•¸ | å…§å­˜ä½¿ç”¨ | å•Ÿå‹•æ™‚é–“ | åˆ†æè³ªé‡ |
|------|----------|----------|----------|----------|
| **ç°¡åŒ–ç‰ˆ** | ~400è¡Œ | <50MB | 2ç§’ | â­â­â­â­â­ |
| åŸè¤‡é›œç‰ˆ | 2500+è¡Œ | >500MB | 30ç§’+ | â­â­â­ |

## ğŸ› ï¸ æœ¬åœ°æ¨¡å‹æ¨è–¦

ä½¿ç”¨Ollamaé‹è¡Œæœ¬åœ°æ¨¡å‹ï¼Œå®Œå…¨å…è²»ä¸”éš±ç§ä¿è­·ï¼š

```bash
# å®‰è£Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# ä¸‹è¼‰æ¨è–¦æ¨¡å‹
ollama pull llama2:13b        # æœ€ä½³å¹³è¡¡
ollama pull mistral:7b        # é€Ÿåº¦å„ªå…ˆ
ollama pull codellama:7b      # æŠ€è¡“åˆ†æ
```

## ğŸš¨ æ•…éšœæ’é™¤

### å¸¸è¦‹å•é¡Œ

1. **No module named 'openai'**
   ```bash
   pip install openai
   ```

2. **Ollamaé€£æ¥å¤±æ•—**
   ```bash
   # ç¢ºä¿Ollamaæœå‹™é‹è¡Œ
   ollama serve
   ```

3. **APIå¯†é‘°éŒ¯èª¤**
   ```bash
   # æª¢æŸ¥ç’°å¢ƒè®Šé‡
   echo $OPENAI_API_KEY
   ```

4. **åˆ†æè³ªé‡ä¸ä½³**
   ```bash
   # æé«˜è³ªé‡é–¾å€¼
   python hr_ai_quality.py analyze-employee data.json --quality-threshold 0.8
   ```

## ğŸ“ é–‹ç™¼æŒ‡å—

### æ“´å±•åˆ†ææ¨¡æ¿

```python
# åœ¨ HRPromptTemplates é¡ä¸­æ·»åŠ æ–°æ¨¡æ¿
@staticmethod
def custom_analysis_prompt(data):
    return "æ‚¨çš„è‡ªå®šç¾©åˆ†ææç¤º..."
```

### æ·»åŠ æ–°çš„LLMæä¾›å•†

```python
# åœ¨ LLMProvider æšèˆ‰ä¸­æ·»åŠ 
NEW_PROVIDER = "new_provider"

# åœ¨ HighQualityLLMClient ä¸­å¯¦ç¾
def _setup_new_provider(self):
    # å¯¦ç¾è¨­ç½®é‚è¼¯
    pass
```

## ğŸ“„ æˆæ¬Šæ¢æ¬¾

MIT License - å¯è‡ªç”±ä½¿ç”¨ã€ä¿®æ”¹å’Œåˆ†ç™¼

## ğŸ¤ è²¢ç»æŒ‡å—

æ­¡è¿æäº¤Issueå’ŒPull Requestï¼

1. Forkæœ¬å°ˆæ¡ˆ
2. å‰µå»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/amazing-feature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add amazing feature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/amazing-feature`)
5. é–‹å•ŸPull Request

---

**è®“HRåˆ†æè®Šå¾—ç°¡å–®è€Œå°ˆæ¥­ï¼** ğŸ¯